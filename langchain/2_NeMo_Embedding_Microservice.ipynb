{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80a6bfd6-2448-4fdb-bdb7-7aa0e077d264",
   "metadata": {},
   "source": [
    "# Using NeMo Retriever Embedding Microservice with LCEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027bf7f5-e8f5-4ec0-be3f-2c4f12d3c58c",
   "metadata": {},
   "source": [
    "In LLM and RAG workflows, [embeddings](https://python.langchain.com/docs/modules/data_connection/text_embedding/) transform document text into vectors that capture semantic meaning. This enables efficient search for contextually relevant documents based on a user's query. These documents are then provided to the LLM, enhancing its ability to generate accurate responses. \n",
    "\n",
    "This notebook will first show how to generate embeddings from a query. Then, we'll use this this approach embedding a document, store the embeddings in a vector store, and use that in a LCEL chain to help the LLM answer a question about the NVIDIA H200 from the first notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2717efc6-73f8-46df-b85f-2fc791eb0cc0",
   "metadata": {},
   "source": [
    "### Generate Embeddings with NeMo Retriever Embedding Microservice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8eda54db-fcbe-4176-b643-5bafcc61b606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "\n",
    "# # # NeMo Retriever Embeddings Microservice\n",
    "# # embedding_model = NVIDIAEmbeddings(base_url=\"http://localhost:8080/v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42ec7d96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('../.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7078750-3063-436d-8ba2-796f7667bccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples of other embedding models \n",
    "\n",
    "import os\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# NVIDIA AI Foundation Models\n",
    "embedding_model = NVIDIAEmbeddings(model=\"NV-Embed-QA\")\n",
    "\n",
    "# HuggingFace Embeddings\n",
    "# embedding_model = HuggingFaceEmbeddings(\n",
    "#     model_name=\"intfloat/e5-large-v2\",\n",
    "#     model_kwargs={\"device\": \"gpu\"},\n",
    "#     encode_kwargs={\"normalize_embeddings\": False},\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2d72596-8acb-44bf-a234-4b43ab3d92fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.036102294921875,\n",
       " -0.04345703125,\n",
       " 0.031890869140625,\n",
       " -0.03680419921875,\n",
       " 0.045867919921875,\n",
       " 0.006679534912109375,\n",
       " -0.0019245147705078125,\n",
       " -0.047119140625,\n",
       " -0.016448974609375,\n",
       " -0.0261077880859375]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create vector embeddings of the query\n",
    "\n",
    "embedding_model.embed_query(\"How much memory does the NVIDIA H200 have?\")[:10] # see the first 10 elements of the vector embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cd9c64-0f4d-4769-809d-0bf3ee076e18",
   "metadata": {},
   "source": [
    "### Load PDF (NVIDIA H200 Datasheet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730338af-f83b-4129-81e0-6352da0c2871",
   "metadata": {},
   "source": [
    "Next, we'll load a PDF of the [NVIDIA H200 Datasheet](https://nvdam.widen.net/s/nb5zzzsjdf/hpc-datasheet-sc23-h200-datasheet-3002446), this is the knowledge base that the LLM will use to retrieve relevant information to answer our question.\n",
    "\n",
    "LangChain provides a variety of [document loaders](https://python.langchain.com/docs/integrations/document_loaders) that load various types of documents (HTML, PDF, code) from many different sources and locations (private s3 buckets, public websites).  [Here](https://python.langchain.com/docs/integrations/document_loaders) are some of the document loaders available from LangChain.\n",
    "\n",
    "In this example, we use a LangChain [`PyPDFLoader`](https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.pdf.PyPDFLoader.html) to load a datasheet about the NVIDIA H200 Tensor Core GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfceb622-8733-481c-98ee-d539e776059a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='NVIDIA H200 Tensor Core GPU\\u2002|\\u2002Datasheet\\u2002|\\u2002 1NVIDIA H200 Tensor Core GPU\\nSupercharging AI and HPC workloads.\\nHigher Performance With Larger, Faster Memory\\nThe NVIDIA H200 Tensor Core GPU supercharges generative AI and high-\\nperformance computing (HPC) workloads with game-changing performance  \\nand memory capabilities. \\nBased on the NVIDIA Hopper™ architecture , the NVIDIA H200 is the first GPU to \\noffer 141 gigabytes (GB) of HBM3e memory at 4.8 terabytes per second (TB/s)—\\nthat’s nearly double the capacity of the NVIDIA H100 Tensor Core GPU  with \\n1.4X more memory bandwidth. The H200’s larger and faster memory accelerates \\ngenerative AI and large language models, while advancing scientific computing for \\nHPC workloads with better energy efficiency and lower total cost of ownership. \\nUnlock Insights With High-Performance LLM Inference\\nIn the ever-evolving landscape of AI, businesses rely on large language models to \\naddress a diverse range of inference needs. An AI inference  accelerator must deliver the \\nhighest throughput at the lowest TCO when deployed at scale for a massive user base. \\nThe H200 doubles inference performance compared to H100 GPUs when handling \\nlarge language models such as Llama2 70B.\\n.\\nPreliminary specifications. May be subject to change.\\nLlama2 13B: ISL 128, OSL 2K | Throughput | H100 SXM 1x GPU BS 64 | H200 SXM 1x GPU BS 128\\nGPT-3 175B: ISL 80, OSL 200 | x8 H100 SXM GPUs BS 64 | x8 H200 SXM GPUs BS 128\\nLlama2 70B: ISL 2K, OSL 128 | Throughput | H100 SXM 1x GPU BS 8 | H200 SXM 1x GPU BS 32.Key Features\\n >141GB of HBM3e GPU memory\\n >4.8TB/s of memory bandwidth\\n >4 petaFLOPS of FP8 performance\\n >2X LLM inference performance\\n >110X HPC performance\\nDatasheet', metadata={'source': 'https://nvdam.widen.net/content/udc6mzrk7a/original/hpc-datasheet-sc23-h200-datasheet-3002446.pdf', 'page': 0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"https://nvdam.widen.net/content/udc6mzrk7a/original/hpc-datasheet-sc23-h200-datasheet-3002446.pdf\")\n",
    "\n",
    "document = loader.load()\n",
    "document[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ff27c4-7b62-42f5-b3ca-602bf513868b",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Once documents have been loaded, they are often transformed. One method of transformation is known as **chunking**, which breaks down large pieces of text, for example, a long document, into smaller segments. This technique is valuable because it helps [optimize the relevance of the content returned from the vector database](https://www.pinecone.io/learn/chunking-strategies/). \n",
    "\n",
    "LangChain provides a [variety of document transformers](https://python.langchain.com/docs/integrations/document_transformers/), such as text splitters. In this example, we use a [``RecursiveCharacterTextSplitter``](https://api.python.langchain.com/en/latest/text_splitter/langchain.text_splitter.RecursiveCharacterTextSplitter.html). The ``RecursiveCharacterTextSplitter`` is designed to divide a large text into smaller chunks based on a specified chunk size. It employs recursion as its core mechanism for splitting text, utilizing a predefined set of characters (e.g., \"\\n\\n\", \"\\n\", \" \", \"\") to determine where splits should occur. The process begins by attempting to split the text using the first character in the set. If the resulting chunks are still larger than the desired chunk size, it proceeds to the next character in the set and attempts to split again. This process continues until all chunks adhere to the specified maximum chunk size.\n",
    "\n",
    "There are some nuanced complexities to text splitting since semantically related text, in theory, should be kept together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "085e80e7-aff3-4b8e-a29f-f000b970ec62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks from the document: 16\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \";\", \",\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "document_chunks = text_splitter.split_documents(document)\n",
    "print(\"Number of chunks from the document:\", len(document_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dcca647-6f93-483c-9c9c-01f9b56e5cc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'offer 141 gigabytes (GB) of HBM3e memory at 4.8 terabytes per second (TB/s)—\\nthat’s nearly double the capacity of the NVIDIA H100 Tensor Core GPU  with \\n1.4X more memory bandwidth. The H200’s larger and faster memory accelerates \\ngenerative AI and large language models, while advancing scientific computing for \\nHPC workloads with better energy efficiency and lower total cost of ownership. \\nUnlock Insights With High-Performance LLM Inference'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get example of a document chunk\n",
    "\n",
    "example_chunk = document_chunks[1].page_content\n",
    "example_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "faa6e0c7-a17e-4b86-9850-50600fd9648b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.0238189697265625,\n",
       " -0.004711151123046875,\n",
       " 0.027740478515625,\n",
       " -0.069580078125,\n",
       " 0.040924072265625,\n",
       " 0.007671356201171875,\n",
       " 0.042816162109375,\n",
       " -0.0144195556640625,\n",
       " 0.02264404296875,\n",
       " -0.0203094482421875]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create vector embeddings of the example chunk\n",
    "\n",
    "embedding_model.embed_query(example_chunk)[:10] # see the first 10 elements of the vector embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05616f71-ed2d-4723-ad17-0e8ce21e2c98",
   "metadata": {},
   "source": [
    "### Store document embeddings in the vector store.\n",
    "\n",
    "Once the document embeddings are generated, they are stored in a vector store so that at query time we can:\n",
    "\n",
    "<ol>\n",
    "    <li>Embed the user query and</li>\n",
    "    <li>Retrieve the embedding vectors that are most similar or relevant to the embedding query.</li>\n",
    "</ol>\n",
    "\n",
    "A vector store takes care of storing the embedded data and performing a vector search. LangChain provides support for a [great selection of vector stores](https://python.langchain.com/docs/integrations/vectorstores/), we'll be using FAISS for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dd482c3-7e93-447d-9f2c-6ab076209300",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Create FAISS vector store from our embedding service \n",
    "vector_store = FAISS.from_documents(document_chunks, embedding=embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95fd5c0-8882-4fad-bb0f-ae274ec906aa",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Next, we'll need to integrate the vector database with the LLM. A [LangChain Expression Language (LCEL)](https://python.langchain.com/docs/modules/chains/) combines these components together. We can then formulate the prompt placeholders (context and question) and pipe it to our LLM connector as shown below to answer the original question from the first notebook (`How much memory does the NVIDIA H200 have?`) with embeddings from the `NVIDIA H200 datasheet` document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d7fd97d-1dc3-442d-9a68-18d8e36b40e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sagdesai/Desktop/work/workshop2/.venv/lib/python3.10/site-packages/langchain_nvidia_ai_endpoints/_common.py:537: UserWarning: Found meta/llama-3.1-8b-instruct in available_models, but type is unknown and inference may fail.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "# Initialize LLM from NVIDIA AI Foundation Endpoints\n",
    "# os.environ[\"NVIDIA_API_KEY\"] = \"nvapi-***\" \n",
    "llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\")\n",
    "\n",
    "# Initialize with NVIDIA NIM for LLMs\n",
    "# llm = ChatNVIDIA(base_url=\"http://localhost:8000/v1\", model=\"meta/llama3-8b-instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969d22c7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "128b9999-a427-4e75-9a33-8170f3d3a3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \n",
    "        \"You are a helpful and friendly AI!\"\n",
    "        \"Your responses should be concise and no longer than two sentences.\"\n",
    "        \"Do not hallucinate. Say you don't know if you don't have this information.\"\n",
    "        # \"Answer the question using only the context\"\n",
    "        \"\\n\\nQuestion: {question}\\n\\nContext: {context}\"\n",
    "    ),\n",
    "    (\"user\", \"{question}\")\n",
    "])\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": vector_store.as_retriever(),\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "324a1d62-421a-4cb8-9146-6aa2668da7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The NVIDIA H200 has 141 gigabytes (GB) of HBM3e memory.\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke(\"How much memory does the NVIDIA H200 have?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7651934-2940-4ee4-8c57-b586768918f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The NVIDIA H200 is based on both PCIe and SXM interfaces, as mentioned in the document it can come in multiple form factors, which includes PCIe and SXM.\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke(\"Is the NVIDIA H200 PCIe or SXM based?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2d77ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
